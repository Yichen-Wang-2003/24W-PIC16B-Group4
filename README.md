
# 24W-PIC16B-Group4

Recurrent Neural Networks Intro:

Recurrent Neural Networks (RNN) introduces “a transition weight to send information over time”, where the hidden layers act “as an internal storage of the information captured in the earlier stages” (Zouaoui et al. 2023). Two powerful RNN models, LSTM and GRU, perform ideal results in application domains with time series.

Long short-term memory (LSTM) model:
LSTM is “a sophisticated gated memory unit designed to mitigate the vanishing gradient problems limiting the efficiency of a simple RNN” (Zeroual et al. 2020). LSTM has “four components: input gates, forget gate, cell state and output gate” (Zouaoui 8).

Gated Recurrent Unit (GRU) model:
GRU aimed to “solve the vanishing gradient problem” and “does not have the cell state and the output gate; thus has fewer parameters” (Zouaoui 8-9)

Model Performance Evaluation Metrics for Forecast Errors:
Mean square error (MSE), Root Means Square Error(RMSE), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), and R-squared.


