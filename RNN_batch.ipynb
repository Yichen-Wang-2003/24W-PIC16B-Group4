{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor \n",
    "import sqlite3\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('df_2023_h1_feature',), ('df_2023_h1_target',)]\n"
     ]
    }
   ],
   "source": [
    "# Read in the data from the database\n",
    "conn = sqlite3.connect('data/tables.db')\n",
    "# show database content\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(cursor.fetchall())\n",
    "# Extract two tables from it and store them in two pd df\n",
    "ds = pd.read_sql_query(\"SELECT * from df_2023_h1_feature\", conn)\n",
    "target = pd.read_sql_query(\"SELECT * from df_2023_h1_target\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[QUOTE_UNIXTIME]</th>\n",
       "      <th>[EXPIRE_UNIX]</th>\n",
       "      <th>[STRIKE]</th>\n",
       "      <th>[UNDERLYING_LAST]</th>\n",
       "      <th>[C_DELTA]</th>\n",
       "      <th>[C_GAMMA]</th>\n",
       "      <th>[C_VEGA]</th>\n",
       "      <th>[C_THETA]</th>\n",
       "      <th>[C_RHO]</th>\n",
       "      <th>[C_IV]</th>\n",
       "      <th>...</th>\n",
       "      <th>[C_ASK]</th>\n",
       "      <th>[P_DELTA]</th>\n",
       "      <th>[P_GAMMA]</th>\n",
       "      <th>[P_VEGA]</th>\n",
       "      <th>[P_THETA]</th>\n",
       "      <th>[P_RHO]</th>\n",
       "      <th>[P_IV]</th>\n",
       "      <th>[P_VOLUME]</th>\n",
       "      <th>[P_BID]</th>\n",
       "      <th>[P_ASK]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.69160</td>\n",
       "      <td>-1.531564</td>\n",
       "      <td>-1.054517</td>\n",
       "      <td>-2.406592</td>\n",
       "      <td>1.054125</td>\n",
       "      <td>-0.052714</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>-0.391189</td>\n",
       "      <td>-0.307878</td>\n",
       "      <td>11.711458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676304</td>\n",
       "      <td>1.120791</td>\n",
       "      <td>-0.013192</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>0.086874</td>\n",
       "      <td>0.592295</td>\n",
       "      <td>3.380989</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.563368</td>\n",
       "      <td>-0.566211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.69160</td>\n",
       "      <td>-1.531564</td>\n",
       "      <td>-0.936052</td>\n",
       "      <td>-2.406592</td>\n",
       "      <td>1.041020</td>\n",
       "      <td>0.036229</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>-0.390353</td>\n",
       "      <td>-0.308470</td>\n",
       "      <td>10.342762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.479078</td>\n",
       "      <td>1.120350</td>\n",
       "      <td>-0.013031</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.086843</td>\n",
       "      <td>0.592878</td>\n",
       "      <td>2.599981</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.563368</td>\n",
       "      <td>-0.566211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.69160</td>\n",
       "      <td>-1.531564</td>\n",
       "      <td>-0.888665</td>\n",
       "      <td>-2.406592</td>\n",
       "      <td>1.035470</td>\n",
       "      <td>0.083539</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>-0.393217</td>\n",
       "      <td>-0.308536</td>\n",
       "      <td>9.795964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400187</td>\n",
       "      <td>1.119196</td>\n",
       "      <td>-0.013111</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.086442</td>\n",
       "      <td>0.592606</td>\n",
       "      <td>2.292638</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.563368</td>\n",
       "      <td>-0.566211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.69160</td>\n",
       "      <td>-1.531564</td>\n",
       "      <td>-0.876819</td>\n",
       "      <td>-2.406592</td>\n",
       "      <td>1.048893</td>\n",
       "      <td>0.103725</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.031159</td>\n",
       "      <td>-0.308217</td>\n",
       "      <td>9.553426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389734</td>\n",
       "      <td>1.120644</td>\n",
       "      <td>-0.013015</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.086741</td>\n",
       "      <td>0.591011</td>\n",
       "      <td>2.214716</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.563368</td>\n",
       "      <td>-0.566211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.69160</td>\n",
       "      <td>-1.531564</td>\n",
       "      <td>-0.864972</td>\n",
       "      <td>-2.406592</td>\n",
       "      <td>1.030873</td>\n",
       "      <td>0.108141</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>-0.438792</td>\n",
       "      <td>-0.308574</td>\n",
       "      <td>9.529689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.371195</td>\n",
       "      <td>1.119712</td>\n",
       "      <td>-0.013079</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.086482</td>\n",
       "      <td>0.591167</td>\n",
       "      <td>2.139460</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.563368</td>\n",
       "      <td>-0.566211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331604</th>\n",
       "      <td>1.78216</td>\n",
       "      <td>1.881977</td>\n",
       "      <td>0.367073</td>\n",
       "      <td>1.410238</td>\n",
       "      <td>-0.175447</td>\n",
       "      <td>0.125803</td>\n",
       "      <td>0.058474</td>\n",
       "      <td>0.064804</td>\n",
       "      <td>0.473889</td>\n",
       "      <td>-0.825637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.386156</td>\n",
       "      <td>-0.261286</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>0.057390</td>\n",
       "      <td>0.064104</td>\n",
       "      <td>-2.657456</td>\n",
       "      <td>-0.483741</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.191656</td>\n",
       "      <td>-0.130110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331605</th>\n",
       "      <td>1.78216</td>\n",
       "      <td>1.881977</td>\n",
       "      <td>0.426306</td>\n",
       "      <td>1.410238</td>\n",
       "      <td>-0.288188</td>\n",
       "      <td>0.141573</td>\n",
       "      <td>0.057498</td>\n",
       "      <td>0.121952</td>\n",
       "      <td>0.401497</td>\n",
       "      <td>-0.825637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.472936</td>\n",
       "      <td>-0.405104</td>\n",
       "      <td>0.004291</td>\n",
       "      <td>0.054827</td>\n",
       "      <td>0.067442</td>\n",
       "      <td>-2.681954</td>\n",
       "      <td>-0.504299</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.142171</td>\n",
       "      <td>-0.085819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331606</th>\n",
       "      <td>1.78216</td>\n",
       "      <td>1.881977</td>\n",
       "      <td>0.485538</td>\n",
       "      <td>1.410238</td>\n",
       "      <td>-0.406236</td>\n",
       "      <td>0.147881</td>\n",
       "      <td>0.055627</td>\n",
       "      <td>0.179100</td>\n",
       "      <td>0.323910</td>\n",
       "      <td>-0.825637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.521651</td>\n",
       "      <td>-0.565709</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.050409</td>\n",
       "      <td>0.071071</td>\n",
       "      <td>-2.573385</td>\n",
       "      <td>-0.528800</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.090211</td>\n",
       "      <td>-0.033009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331607</th>\n",
       "      <td>1.78216</td>\n",
       "      <td>1.881977</td>\n",
       "      <td>0.544771</td>\n",
       "      <td>1.410238</td>\n",
       "      <td>-0.524528</td>\n",
       "      <td>0.142835</td>\n",
       "      <td>0.052804</td>\n",
       "      <td>0.249133</td>\n",
       "      <td>0.243663</td>\n",
       "      <td>-0.825637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.564252</td>\n",
       "      <td>-0.714509</td>\n",
       "      <td>0.005499</td>\n",
       "      <td>0.044680</td>\n",
       "      <td>0.074684</td>\n",
       "      <td>-2.371958</td>\n",
       "      <td>-0.536906</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.024928</td>\n",
       "      <td>0.033049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331608</th>\n",
       "      <td>1.78216</td>\n",
       "      <td>1.881977</td>\n",
       "      <td>0.604004</td>\n",
       "      <td>1.410238</td>\n",
       "      <td>-0.647832</td>\n",
       "      <td>0.128957</td>\n",
       "      <td>0.048808</td>\n",
       "      <td>0.324893</td>\n",
       "      <td>0.159893</td>\n",
       "      <td>-0.825637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.599752</td>\n",
       "      <td>-0.877274</td>\n",
       "      <td>0.005515</td>\n",
       "      <td>0.036596</td>\n",
       "      <td>0.078698</td>\n",
       "      <td>-1.955998</td>\n",
       "      <td>-0.553411</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.104029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>331609 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        [QUOTE_UNIXTIME]  [EXPIRE_UNIX]  [STRIKE]  [UNDERLYING_LAST]  \\\n",
       "0               -1.69160      -1.531564 -1.054517          -2.406592   \n",
       "1               -1.69160      -1.531564 -0.936052          -2.406592   \n",
       "2               -1.69160      -1.531564 -0.888665          -2.406592   \n",
       "3               -1.69160      -1.531564 -0.876819          -2.406592   \n",
       "4               -1.69160      -1.531564 -0.864972          -2.406592   \n",
       "...                  ...            ...       ...                ...   \n",
       "331604           1.78216       1.881977  0.367073           1.410238   \n",
       "331605           1.78216       1.881977  0.426306           1.410238   \n",
       "331606           1.78216       1.881977  0.485538           1.410238   \n",
       "331607           1.78216       1.881977  0.544771           1.410238   \n",
       "331608           1.78216       1.881977  0.604004           1.410238   \n",
       "\n",
       "        [C_DELTA]  [C_GAMMA]  [C_VEGA]  [C_THETA]   [C_RHO]     [C_IV]  ...  \\\n",
       "0        1.054125  -0.052714  0.000146  -0.391189 -0.307878  11.711458  ...   \n",
       "1        1.041020   0.036229  0.000190  -0.390353 -0.308470  10.342762  ...   \n",
       "2        1.035470   0.083539  0.000199  -0.393217 -0.308536   9.795964  ...   \n",
       "3        1.048893   0.103725  0.000140   0.031159 -0.308217   9.553426  ...   \n",
       "4        1.030873   0.108141  0.000225  -0.438792 -0.308574   9.529689  ...   \n",
       "...           ...        ...       ...        ...       ...        ...  ...   \n",
       "331604  -0.175447   0.125803  0.058474   0.064804  0.473889  -0.825637  ...   \n",
       "331605  -0.288188   0.141573  0.057498   0.121952  0.401497  -0.825637  ...   \n",
       "331606  -0.406236   0.147881  0.055627   0.179100  0.323910  -0.825637  ...   \n",
       "331607  -0.524528   0.142835  0.052804   0.249133  0.243663  -0.825637  ...   \n",
       "331608  -0.647832   0.128957  0.048808   0.324893  0.159893  -0.825637  ...   \n",
       "\n",
       "         [C_ASK]  [P_DELTA]  [P_GAMMA]  [P_VEGA]  [P_THETA]   [P_RHO]  \\\n",
       "0       0.676304   1.120791  -0.013192  0.001135   0.086874  0.592295   \n",
       "1       0.479078   1.120350  -0.013031  0.001152   0.086843  0.592878   \n",
       "2       0.400187   1.119196  -0.013111  0.001152   0.086442  0.592606   \n",
       "3       0.389734   1.120644  -0.013015  0.001157   0.086741  0.591011   \n",
       "4       0.371195   1.119712  -0.013079  0.001120   0.086482  0.591167   \n",
       "...          ...        ...        ...       ...        ...       ...   \n",
       "331604 -0.386156  -0.261286   0.003340  0.057390   0.064104 -2.657456   \n",
       "331605 -0.472936  -0.405104   0.004291  0.054827   0.067442 -2.681954   \n",
       "331606 -0.521651  -0.565709   0.005225  0.050409   0.071071 -2.573385   \n",
       "331607 -0.564252  -0.714509   0.005499  0.044680   0.074684 -2.371958   \n",
       "331608 -0.599752  -0.877274   0.005515  0.036596   0.078698 -1.955998   \n",
       "\n",
       "          [P_IV]  [P_VOLUME]   [P_BID]   [P_ASK]  \n",
       "0       3.380989   -0.100406 -0.563368 -0.566211  \n",
       "1       2.599981   -0.100406 -0.563368 -0.566211  \n",
       "2       2.292638   -0.100406 -0.563368 -0.566211  \n",
       "3       2.214716   -0.100406 -0.563368 -0.566211  \n",
       "4       2.139460   -0.100406 -0.563368 -0.566211  \n",
       "...          ...         ...       ...       ...  \n",
       "331604 -0.483741   -0.100406 -0.191656 -0.130110  \n",
       "331605 -0.504299   -0.100406 -0.142171 -0.085819  \n",
       "331606 -0.528800   -0.100406 -0.090211 -0.033009  \n",
       "331607 -0.536906   -0.100406 -0.024928  0.033049  \n",
       "331608 -0.553411   -0.100406  0.044922  0.104029  \n",
       "\n",
       "[331609 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_new = ds.copy()\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMMM, we first standardize two cols \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "# ignore all caught warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    scaler = StandardScaler()\n",
    "    tbd = ['[QUOTE_UNIXTIME]', '[EXPIRE_UNIX]']\n",
    "    ds_new[tbd] = scaler.fit_transform(ds[tbd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[QUOTE_UNIXTIME]</th>\n",
       "      <th>[EXPIRE_UNIX]</th>\n",
       "      <th>[STRIKE]</th>\n",
       "      <th>[UNDERLYING_LAST]</th>\n",
       "      <th>[C_DELTA]</th>\n",
       "      <th>[C_GAMMA]</th>\n",
       "      <th>[C_VEGA]</th>\n",
       "      <th>[C_THETA]</th>\n",
       "      <th>[C_RHO]</th>\n",
       "      <th>[C_IV]</th>\n",
       "      <th>...</th>\n",
       "      <th>[C_ASK]</th>\n",
       "      <th>[P_DELTA]</th>\n",
       "      <th>[P_GAMMA]</th>\n",
       "      <th>[P_VEGA]</th>\n",
       "      <th>[P_THETA]</th>\n",
       "      <th>[P_RHO]</th>\n",
       "      <th>[P_IV]</th>\n",
       "      <th>[P_VOLUME]</th>\n",
       "      <th>[P_BID]</th>\n",
       "      <th>[P_ASK]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.69160</td>\n",
       "      <td>-1.531564</td>\n",
       "      <td>-1.054517</td>\n",
       "      <td>-2.406592</td>\n",
       "      <td>1.054125</td>\n",
       "      <td>-0.052714</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>-0.391189</td>\n",
       "      <td>-0.307878</td>\n",
       "      <td>11.711458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676304</td>\n",
       "      <td>1.120791</td>\n",
       "      <td>-0.013192</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>0.086874</td>\n",
       "      <td>0.592295</td>\n",
       "      <td>3.380989</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.563368</td>\n",
       "      <td>-0.566211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.69160</td>\n",
       "      <td>-1.531564</td>\n",
       "      <td>-0.936052</td>\n",
       "      <td>-2.406592</td>\n",
       "      <td>1.041020</td>\n",
       "      <td>0.036229</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>-0.390353</td>\n",
       "      <td>-0.308470</td>\n",
       "      <td>10.342762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.479078</td>\n",
       "      <td>1.120350</td>\n",
       "      <td>-0.013031</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.086843</td>\n",
       "      <td>0.592878</td>\n",
       "      <td>2.599981</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.563368</td>\n",
       "      <td>-0.566211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.69160</td>\n",
       "      <td>-1.531564</td>\n",
       "      <td>-0.888665</td>\n",
       "      <td>-2.406592</td>\n",
       "      <td>1.035470</td>\n",
       "      <td>0.083539</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>-0.393217</td>\n",
       "      <td>-0.308536</td>\n",
       "      <td>9.795964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400187</td>\n",
       "      <td>1.119196</td>\n",
       "      <td>-0.013111</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.086442</td>\n",
       "      <td>0.592606</td>\n",
       "      <td>2.292638</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.563368</td>\n",
       "      <td>-0.566211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.69160</td>\n",
       "      <td>-1.531564</td>\n",
       "      <td>-0.876819</td>\n",
       "      <td>-2.406592</td>\n",
       "      <td>1.048893</td>\n",
       "      <td>0.103725</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.031159</td>\n",
       "      <td>-0.308217</td>\n",
       "      <td>9.553426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389734</td>\n",
       "      <td>1.120644</td>\n",
       "      <td>-0.013015</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.086741</td>\n",
       "      <td>0.591011</td>\n",
       "      <td>2.214716</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.563368</td>\n",
       "      <td>-0.566211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.69160</td>\n",
       "      <td>-1.531564</td>\n",
       "      <td>-0.864972</td>\n",
       "      <td>-2.406592</td>\n",
       "      <td>1.030873</td>\n",
       "      <td>0.108141</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>-0.438792</td>\n",
       "      <td>-0.308574</td>\n",
       "      <td>9.529689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.371195</td>\n",
       "      <td>1.119712</td>\n",
       "      <td>-0.013079</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.086482</td>\n",
       "      <td>0.591167</td>\n",
       "      <td>2.139460</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.563368</td>\n",
       "      <td>-0.566211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331604</th>\n",
       "      <td>1.78216</td>\n",
       "      <td>1.881977</td>\n",
       "      <td>0.367073</td>\n",
       "      <td>1.410238</td>\n",
       "      <td>-0.175447</td>\n",
       "      <td>0.125803</td>\n",
       "      <td>0.058474</td>\n",
       "      <td>0.064804</td>\n",
       "      <td>0.473889</td>\n",
       "      <td>-0.825637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.386156</td>\n",
       "      <td>-0.261286</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>0.057390</td>\n",
       "      <td>0.064104</td>\n",
       "      <td>-2.657456</td>\n",
       "      <td>-0.483741</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.191656</td>\n",
       "      <td>-0.130110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331605</th>\n",
       "      <td>1.78216</td>\n",
       "      <td>1.881977</td>\n",
       "      <td>0.426306</td>\n",
       "      <td>1.410238</td>\n",
       "      <td>-0.288188</td>\n",
       "      <td>0.141573</td>\n",
       "      <td>0.057498</td>\n",
       "      <td>0.121952</td>\n",
       "      <td>0.401497</td>\n",
       "      <td>-0.825637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.472936</td>\n",
       "      <td>-0.405104</td>\n",
       "      <td>0.004291</td>\n",
       "      <td>0.054827</td>\n",
       "      <td>0.067442</td>\n",
       "      <td>-2.681954</td>\n",
       "      <td>-0.504299</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.142171</td>\n",
       "      <td>-0.085819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331606</th>\n",
       "      <td>1.78216</td>\n",
       "      <td>1.881977</td>\n",
       "      <td>0.485538</td>\n",
       "      <td>1.410238</td>\n",
       "      <td>-0.406236</td>\n",
       "      <td>0.147881</td>\n",
       "      <td>0.055627</td>\n",
       "      <td>0.179100</td>\n",
       "      <td>0.323910</td>\n",
       "      <td>-0.825637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.521651</td>\n",
       "      <td>-0.565709</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.050409</td>\n",
       "      <td>0.071071</td>\n",
       "      <td>-2.573385</td>\n",
       "      <td>-0.528800</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.090211</td>\n",
       "      <td>-0.033009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331607</th>\n",
       "      <td>1.78216</td>\n",
       "      <td>1.881977</td>\n",
       "      <td>0.544771</td>\n",
       "      <td>1.410238</td>\n",
       "      <td>-0.524528</td>\n",
       "      <td>0.142835</td>\n",
       "      <td>0.052804</td>\n",
       "      <td>0.249133</td>\n",
       "      <td>0.243663</td>\n",
       "      <td>-0.825637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.564252</td>\n",
       "      <td>-0.714509</td>\n",
       "      <td>0.005499</td>\n",
       "      <td>0.044680</td>\n",
       "      <td>0.074684</td>\n",
       "      <td>-2.371958</td>\n",
       "      <td>-0.536906</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.024928</td>\n",
       "      <td>0.033049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331608</th>\n",
       "      <td>1.78216</td>\n",
       "      <td>1.881977</td>\n",
       "      <td>0.604004</td>\n",
       "      <td>1.410238</td>\n",
       "      <td>-0.647832</td>\n",
       "      <td>0.128957</td>\n",
       "      <td>0.048808</td>\n",
       "      <td>0.324893</td>\n",
       "      <td>0.159893</td>\n",
       "      <td>-0.825637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.599752</td>\n",
       "      <td>-0.877274</td>\n",
       "      <td>0.005515</td>\n",
       "      <td>0.036596</td>\n",
       "      <td>0.078698</td>\n",
       "      <td>-1.955998</td>\n",
       "      <td>-0.553411</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.104029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>331609 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        [QUOTE_UNIXTIME]  [EXPIRE_UNIX]  [STRIKE]  [UNDERLYING_LAST]  \\\n",
       "0               -1.69160      -1.531564 -1.054517          -2.406592   \n",
       "1               -1.69160      -1.531564 -0.936052          -2.406592   \n",
       "2               -1.69160      -1.531564 -0.888665          -2.406592   \n",
       "3               -1.69160      -1.531564 -0.876819          -2.406592   \n",
       "4               -1.69160      -1.531564 -0.864972          -2.406592   \n",
       "...                  ...            ...       ...                ...   \n",
       "331604           1.78216       1.881977  0.367073           1.410238   \n",
       "331605           1.78216       1.881977  0.426306           1.410238   \n",
       "331606           1.78216       1.881977  0.485538           1.410238   \n",
       "331607           1.78216       1.881977  0.544771           1.410238   \n",
       "331608           1.78216       1.881977  0.604004           1.410238   \n",
       "\n",
       "        [C_DELTA]  [C_GAMMA]  [C_VEGA]  [C_THETA]   [C_RHO]     [C_IV]  ...  \\\n",
       "0        1.054125  -0.052714  0.000146  -0.391189 -0.307878  11.711458  ...   \n",
       "1        1.041020   0.036229  0.000190  -0.390353 -0.308470  10.342762  ...   \n",
       "2        1.035470   0.083539  0.000199  -0.393217 -0.308536   9.795964  ...   \n",
       "3        1.048893   0.103725  0.000140   0.031159 -0.308217   9.553426  ...   \n",
       "4        1.030873   0.108141  0.000225  -0.438792 -0.308574   9.529689  ...   \n",
       "...           ...        ...       ...        ...       ...        ...  ...   \n",
       "331604  -0.175447   0.125803  0.058474   0.064804  0.473889  -0.825637  ...   \n",
       "331605  -0.288188   0.141573  0.057498   0.121952  0.401497  -0.825637  ...   \n",
       "331606  -0.406236   0.147881  0.055627   0.179100  0.323910  -0.825637  ...   \n",
       "331607  -0.524528   0.142835  0.052804   0.249133  0.243663  -0.825637  ...   \n",
       "331608  -0.647832   0.128957  0.048808   0.324893  0.159893  -0.825637  ...   \n",
       "\n",
       "         [C_ASK]  [P_DELTA]  [P_GAMMA]  [P_VEGA]  [P_THETA]   [P_RHO]  \\\n",
       "0       0.676304   1.120791  -0.013192  0.001135   0.086874  0.592295   \n",
       "1       0.479078   1.120350  -0.013031  0.001152   0.086843  0.592878   \n",
       "2       0.400187   1.119196  -0.013111  0.001152   0.086442  0.592606   \n",
       "3       0.389734   1.120644  -0.013015  0.001157   0.086741  0.591011   \n",
       "4       0.371195   1.119712  -0.013079  0.001120   0.086482  0.591167   \n",
       "...          ...        ...        ...       ...        ...       ...   \n",
       "331604 -0.386156  -0.261286   0.003340  0.057390   0.064104 -2.657456   \n",
       "331605 -0.472936  -0.405104   0.004291  0.054827   0.067442 -2.681954   \n",
       "331606 -0.521651  -0.565709   0.005225  0.050409   0.071071 -2.573385   \n",
       "331607 -0.564252  -0.714509   0.005499  0.044680   0.074684 -2.371958   \n",
       "331608 -0.599752  -0.877274   0.005515  0.036596   0.078698 -1.955998   \n",
       "\n",
       "          [P_IV]  [P_VOLUME]   [P_BID]   [P_ASK]  \n",
       "0       3.380989   -0.100406 -0.563368 -0.566211  \n",
       "1       2.599981   -0.100406 -0.563368 -0.566211  \n",
       "2       2.292638   -0.100406 -0.563368 -0.566211  \n",
       "3       2.214716   -0.100406 -0.563368 -0.566211  \n",
       "4       2.139460   -0.100406 -0.563368 -0.566211  \n",
       "...          ...         ...       ...       ...  \n",
       "331604 -0.483741   -0.100406 -0.191656 -0.130110  \n",
       "331605 -0.504299   -0.100406 -0.142171 -0.085819  \n",
       "331606 -0.528800   -0.100406 -0.090211 -0.033009  \n",
       "331607 -0.536906   -0.100406 -0.024928  0.033049  \n",
       "331608 -0.553411   -0.100406  0.044922  0.104029  \n",
       "\n",
       "[331609 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discounted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-63.916774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-54.101350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-50.175181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-49.193639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-48.212096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331604</th>\n",
       "      <td>-18.619560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331605</th>\n",
       "      <td>-13.711848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331606</th>\n",
       "      <td>-8.804136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331607</th>\n",
       "      <td>-3.896425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331608</th>\n",
       "      <td>1.011287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>331609 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        discounted_price\n",
       "0             -63.916774\n",
       "1             -54.101350\n",
       "2             -50.175181\n",
       "3             -49.193639\n",
       "4             -48.212096\n",
       "...                  ...\n",
       "331604        -18.619560\n",
       "331605        -13.711848\n",
       "331606         -8.804136\n",
       "331607         -3.896425\n",
       "331608          1.011287\n",
       "\n",
       "[331609 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge target and ds directly, as they match and have the same length\n",
    "ds_new = pd.concat([ds_new, target], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['[QUOTE_UNIXTIME]', '[EXPIRE_UNIX]', '[STRIKE]', '[UNDERLYING_LAST]',\n",
       "       '[C_DELTA]', '[C_GAMMA]', '[C_VEGA]', '[C_THETA]', '[C_RHO]', '[C_IV]',\n",
       "       '[C_VOLUME]', '[C_BID]', '[C_ASK]', '[P_DELTA]', '[P_GAMMA]',\n",
       "       '[P_VEGA]', '[P_THETA]', '[P_RHO]', '[P_IV]', '[P_VOLUME]', '[P_BID]',\n",
       "       '[P_ASK]', 'discounted_price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ds_new[['[QUOTE_UNIXTIME]', '[EXPIRE_UNIX]', '[STRIKE]', '[UNDERLYING_LAST]', '[C_DELTA]', '[C_GAMMA]', '[C_VEGA]',\n",
    "       '[C_THETA]', '[C_RHO]', '[C_IV]', '[C_VOLUME]','[C_BID]', '[C_ASK]', '[P_DELTA]', '[P_GAMMA]', '[P_VEGA]', '[P_THETA]',\n",
    "       '[P_RHO]', '[P_IV]', '[P_VOLUME]', '[P_BID]', '[P_ASK]']].values\n",
    "target_1= ds_new['discounted_price']\n",
    "seq_length = 10 # Number of days to look back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[QUOTE_UNIXTIME]</th>\n",
       "      <th>[EXPIRE_UNIX]</th>\n",
       "      <th>[STRIKE]</th>\n",
       "      <th>[UNDERLYING_LAST]</th>\n",
       "      <th>[C_DELTA]</th>\n",
       "      <th>[C_GAMMA]</th>\n",
       "      <th>[C_VEGA]</th>\n",
       "      <th>[C_THETA]</th>\n",
       "      <th>[C_RHO]</th>\n",
       "      <th>[C_IV]</th>\n",
       "      <th>...</th>\n",
       "      <th>[P_DELTA]</th>\n",
       "      <th>[P_GAMMA]</th>\n",
       "      <th>[P_VEGA]</th>\n",
       "      <th>[P_THETA]</th>\n",
       "      <th>[P_RHO]</th>\n",
       "      <th>[P_IV]</th>\n",
       "      <th>[P_VOLUME]</th>\n",
       "      <th>[P_BID]</th>\n",
       "      <th>[P_ASK]</th>\n",
       "      <th>discounted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.69160</td>\n",
       "      <td>-1.531564</td>\n",
       "      <td>-1.054517</td>\n",
       "      <td>-2.406592</td>\n",
       "      <td>1.054125</td>\n",
       "      <td>-0.052714</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>-0.391189</td>\n",
       "      <td>-0.307878</td>\n",
       "      <td>11.711458</td>\n",
       "      <td>...</td>\n",
       "      <td>1.120791</td>\n",
       "      <td>-0.013192</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>0.086874</td>\n",
       "      <td>0.592295</td>\n",
       "      <td>3.380989</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.563368</td>\n",
       "      <td>-0.566211</td>\n",
       "      <td>-63.916774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.69160</td>\n",
       "      <td>-1.531564</td>\n",
       "      <td>-0.936052</td>\n",
       "      <td>-2.406592</td>\n",
       "      <td>1.041020</td>\n",
       "      <td>0.036229</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>-0.390353</td>\n",
       "      <td>-0.308470</td>\n",
       "      <td>10.342762</td>\n",
       "      <td>...</td>\n",
       "      <td>1.120350</td>\n",
       "      <td>-0.013031</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.086843</td>\n",
       "      <td>0.592878</td>\n",
       "      <td>2.599981</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.563368</td>\n",
       "      <td>-0.566211</td>\n",
       "      <td>-54.101350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.69160</td>\n",
       "      <td>-1.531564</td>\n",
       "      <td>-0.888665</td>\n",
       "      <td>-2.406592</td>\n",
       "      <td>1.035470</td>\n",
       "      <td>0.083539</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>-0.393217</td>\n",
       "      <td>-0.308536</td>\n",
       "      <td>9.795964</td>\n",
       "      <td>...</td>\n",
       "      <td>1.119196</td>\n",
       "      <td>-0.013111</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.086442</td>\n",
       "      <td>0.592606</td>\n",
       "      <td>2.292638</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.563368</td>\n",
       "      <td>-0.566211</td>\n",
       "      <td>-50.175181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.69160</td>\n",
       "      <td>-1.531564</td>\n",
       "      <td>-0.876819</td>\n",
       "      <td>-2.406592</td>\n",
       "      <td>1.048893</td>\n",
       "      <td>0.103725</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.031159</td>\n",
       "      <td>-0.308217</td>\n",
       "      <td>9.553426</td>\n",
       "      <td>...</td>\n",
       "      <td>1.120644</td>\n",
       "      <td>-0.013015</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.086741</td>\n",
       "      <td>0.591011</td>\n",
       "      <td>2.214716</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.563368</td>\n",
       "      <td>-0.566211</td>\n",
       "      <td>-49.193639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.69160</td>\n",
       "      <td>-1.531564</td>\n",
       "      <td>-0.864972</td>\n",
       "      <td>-2.406592</td>\n",
       "      <td>1.030873</td>\n",
       "      <td>0.108141</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>-0.438792</td>\n",
       "      <td>-0.308574</td>\n",
       "      <td>9.529689</td>\n",
       "      <td>...</td>\n",
       "      <td>1.119712</td>\n",
       "      <td>-0.013079</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.086482</td>\n",
       "      <td>0.591167</td>\n",
       "      <td>2.139460</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.563368</td>\n",
       "      <td>-0.566211</td>\n",
       "      <td>-48.212096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331604</th>\n",
       "      <td>1.78216</td>\n",
       "      <td>1.881977</td>\n",
       "      <td>0.367073</td>\n",
       "      <td>1.410238</td>\n",
       "      <td>-0.175447</td>\n",
       "      <td>0.125803</td>\n",
       "      <td>0.058474</td>\n",
       "      <td>0.064804</td>\n",
       "      <td>0.473889</td>\n",
       "      <td>-0.825637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261286</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>0.057390</td>\n",
       "      <td>0.064104</td>\n",
       "      <td>-2.657456</td>\n",
       "      <td>-0.483741</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.191656</td>\n",
       "      <td>-0.130110</td>\n",
       "      <td>-18.619560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331605</th>\n",
       "      <td>1.78216</td>\n",
       "      <td>1.881977</td>\n",
       "      <td>0.426306</td>\n",
       "      <td>1.410238</td>\n",
       "      <td>-0.288188</td>\n",
       "      <td>0.141573</td>\n",
       "      <td>0.057498</td>\n",
       "      <td>0.121952</td>\n",
       "      <td>0.401497</td>\n",
       "      <td>-0.825637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.405104</td>\n",
       "      <td>0.004291</td>\n",
       "      <td>0.054827</td>\n",
       "      <td>0.067442</td>\n",
       "      <td>-2.681954</td>\n",
       "      <td>-0.504299</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.142171</td>\n",
       "      <td>-0.085819</td>\n",
       "      <td>-13.711848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331606</th>\n",
       "      <td>1.78216</td>\n",
       "      <td>1.881977</td>\n",
       "      <td>0.485538</td>\n",
       "      <td>1.410238</td>\n",
       "      <td>-0.406236</td>\n",
       "      <td>0.147881</td>\n",
       "      <td>0.055627</td>\n",
       "      <td>0.179100</td>\n",
       "      <td>0.323910</td>\n",
       "      <td>-0.825637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.565709</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.050409</td>\n",
       "      <td>0.071071</td>\n",
       "      <td>-2.573385</td>\n",
       "      <td>-0.528800</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.090211</td>\n",
       "      <td>-0.033009</td>\n",
       "      <td>-8.804136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331607</th>\n",
       "      <td>1.78216</td>\n",
       "      <td>1.881977</td>\n",
       "      <td>0.544771</td>\n",
       "      <td>1.410238</td>\n",
       "      <td>-0.524528</td>\n",
       "      <td>0.142835</td>\n",
       "      <td>0.052804</td>\n",
       "      <td>0.249133</td>\n",
       "      <td>0.243663</td>\n",
       "      <td>-0.825637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.714509</td>\n",
       "      <td>0.005499</td>\n",
       "      <td>0.044680</td>\n",
       "      <td>0.074684</td>\n",
       "      <td>-2.371958</td>\n",
       "      <td>-0.536906</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>-0.024928</td>\n",
       "      <td>0.033049</td>\n",
       "      <td>-3.896425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331608</th>\n",
       "      <td>1.78216</td>\n",
       "      <td>1.881977</td>\n",
       "      <td>0.604004</td>\n",
       "      <td>1.410238</td>\n",
       "      <td>-0.647832</td>\n",
       "      <td>0.128957</td>\n",
       "      <td>0.048808</td>\n",
       "      <td>0.324893</td>\n",
       "      <td>0.159893</td>\n",
       "      <td>-0.825637</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.877274</td>\n",
       "      <td>0.005515</td>\n",
       "      <td>0.036596</td>\n",
       "      <td>0.078698</td>\n",
       "      <td>-1.955998</td>\n",
       "      <td>-0.553411</td>\n",
       "      <td>-0.100406</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.104029</td>\n",
       "      <td>1.011287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>331609 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        [QUOTE_UNIXTIME]  [EXPIRE_UNIX]  [STRIKE]  [UNDERLYING_LAST]  \\\n",
       "0               -1.69160      -1.531564 -1.054517          -2.406592   \n",
       "1               -1.69160      -1.531564 -0.936052          -2.406592   \n",
       "2               -1.69160      -1.531564 -0.888665          -2.406592   \n",
       "3               -1.69160      -1.531564 -0.876819          -2.406592   \n",
       "4               -1.69160      -1.531564 -0.864972          -2.406592   \n",
       "...                  ...            ...       ...                ...   \n",
       "331604           1.78216       1.881977  0.367073           1.410238   \n",
       "331605           1.78216       1.881977  0.426306           1.410238   \n",
       "331606           1.78216       1.881977  0.485538           1.410238   \n",
       "331607           1.78216       1.881977  0.544771           1.410238   \n",
       "331608           1.78216       1.881977  0.604004           1.410238   \n",
       "\n",
       "        [C_DELTA]  [C_GAMMA]  [C_VEGA]  [C_THETA]   [C_RHO]     [C_IV]  ...  \\\n",
       "0        1.054125  -0.052714  0.000146  -0.391189 -0.307878  11.711458  ...   \n",
       "1        1.041020   0.036229  0.000190  -0.390353 -0.308470  10.342762  ...   \n",
       "2        1.035470   0.083539  0.000199  -0.393217 -0.308536   9.795964  ...   \n",
       "3        1.048893   0.103725  0.000140   0.031159 -0.308217   9.553426  ...   \n",
       "4        1.030873   0.108141  0.000225  -0.438792 -0.308574   9.529689  ...   \n",
       "...           ...        ...       ...        ...       ...        ...  ...   \n",
       "331604  -0.175447   0.125803  0.058474   0.064804  0.473889  -0.825637  ...   \n",
       "331605  -0.288188   0.141573  0.057498   0.121952  0.401497  -0.825637  ...   \n",
       "331606  -0.406236   0.147881  0.055627   0.179100  0.323910  -0.825637  ...   \n",
       "331607  -0.524528   0.142835  0.052804   0.249133  0.243663  -0.825637  ...   \n",
       "331608  -0.647832   0.128957  0.048808   0.324893  0.159893  -0.825637  ...   \n",
       "\n",
       "        [P_DELTA]  [P_GAMMA]  [P_VEGA]  [P_THETA]   [P_RHO]    [P_IV]  \\\n",
       "0        1.120791  -0.013192  0.001135   0.086874  0.592295  3.380989   \n",
       "1        1.120350  -0.013031  0.001152   0.086843  0.592878  2.599981   \n",
       "2        1.119196  -0.013111  0.001152   0.086442  0.592606  2.292638   \n",
       "3        1.120644  -0.013015  0.001157   0.086741  0.591011  2.214716   \n",
       "4        1.119712  -0.013079  0.001120   0.086482  0.591167  2.139460   \n",
       "...           ...        ...       ...        ...       ...       ...   \n",
       "331604  -0.261286   0.003340  0.057390   0.064104 -2.657456 -0.483741   \n",
       "331605  -0.405104   0.004291  0.054827   0.067442 -2.681954 -0.504299   \n",
       "331606  -0.565709   0.005225  0.050409   0.071071 -2.573385 -0.528800   \n",
       "331607  -0.714509   0.005499  0.044680   0.074684 -2.371958 -0.536906   \n",
       "331608  -0.877274   0.005515  0.036596   0.078698 -1.955998 -0.553411   \n",
       "\n",
       "        [P_VOLUME]   [P_BID]   [P_ASK]  discounted_price  \n",
       "0        -0.100406 -0.563368 -0.566211        -63.916774  \n",
       "1        -0.100406 -0.563368 -0.566211        -54.101350  \n",
       "2        -0.100406 -0.563368 -0.566211        -50.175181  \n",
       "3        -0.100406 -0.563368 -0.566211        -49.193639  \n",
       "4        -0.100406 -0.563368 -0.566211        -48.212096  \n",
       "...            ...       ...       ...               ...  \n",
       "331604   -0.100406 -0.191656 -0.130110        -18.619560  \n",
       "331605   -0.100406 -0.142171 -0.085819        -13.711848  \n",
       "331606   -0.100406 -0.090211 -0.033009         -8.804136  \n",
       "331607   -0.100406 -0.024928  0.033049         -3.896425  \n",
       "331608   -0.100406  0.044922  0.104029          1.011287  \n",
       "\n",
       "[331609 rows x 23 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence creation\n",
    "def seq(data, seq_length):\n",
    "    xseq = []\n",
    "    yseq = []\n",
    "    for i in range(len(data)-seq_length-1):\n",
    "        x = data[i:(i+seq_length)]\n",
    "        y = target_1[i+seq_length]\n",
    "        xseq.append(x)\n",
    "        yseq.append(y)\n",
    "    return np.array(xseq), np.array(yseq)\n",
    "X,y = seq(features, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((331598, 10, 22), (331598,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(331609,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_1_seq = np.array(target_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# First, split the data into training + validation and testing sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Then, split the training + validation into separate training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42)  # 0.1765 is roughly 15% of the original dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([232110, 10, 22]) torch.Size([232110, 1]) torch.Size([49748, 10, 22]) torch.Size([49748, 1]) torch.Size([49740, 10, 22]) torch.Size([49740, 1])\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.reshape(-1, 1)\n",
    "y_val = y_val.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "X_train = X_train.astype(float)  # Convert to float if it's a numpy array\n",
    "X_train = torch.tensor(X_train, dtype=torch.float)  # Then convert to a PyTorch tensor\n",
    "y_train = torch.tensor(y_train, dtype=torch.float)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = y_train.squeeze(1)  # Removes the unnecessary middle dimension\n",
    "# y_val = y_val.squeeze(1)\n",
    "# y_test = y_test.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes: torch.Size([232110, 10, 22]) torch.Size([232110, 1])\n",
      "Validation shapes: torch.Size([49748, 10, 22]) torch.Size([49748, 1])\n",
      "Test shapes: torch.Size([49740, 10, 22]) torch.Size([49740, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shapes:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation shapes:\", X_val.shape, y_val.shape)\n",
    "print(\"Test shapes:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implement batch based on `https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, features, target):\n",
    "        'Initialization'\n",
    "        self.target = target\n",
    "        self.features = features\n",
    "\n",
    "  def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        features = self.features[index, :, :]\n",
    "        target = self.target[index, :]\n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 1,\n",
    "          'shuffle': False}\n",
    "\n",
    "training_set = Dataset(X_train, y_train)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(X_val, y_val)\n",
    "validation_generator = torch.utils.data.DataLoader(validation_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size=22,dropout=0.5):\n",
    "        super(RNN_2, self).__init__()\n",
    "        # hidden layer size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True,dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.fc1 = nn.Linear(output_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        test_h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        # print(f\"test_h0 has shape,{test_h0.shape}\")  # Should work without error and print the shape)\n",
    "        # initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        # initialize cell state with zeros\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        output, _ = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        output = self.relu(output)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "    \n",
    "# Model instantiation\n",
    "model = RNN_2(input_size=X_train.shape[2], hidden_size=50, num_layers=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[191], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val_batch, val_target \u001b[38;5;129;01min\u001b[39;00m validation_generator:\n\u001b[1;32m---> 26\u001b[0m         val_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m         val_loss \u001b[38;5;241m=\u001b[39m criterion(val_outputs, val_target)\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;66;03m# Early stopping\u001b[39;00m\n",
      "File \u001b[1;32md:\\Tools\\Anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Tools\\Anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[188], line 18\u001b[0m, in \u001b[0;36mRNN_2.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# initialize cell state with zeros\u001b[39;00m\n\u001b[0;32m     17\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m---> 18\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[0;32m     20\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(output)\n",
      "File \u001b[1;32md:\\Tools\\Anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Tools\\Anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Tools\\Anaconda3\\envs\\PIC16B-24W\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:878\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    875\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 878\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    882\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1, weight_decay=1e-4)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# For early stopping\n",
    "patience = 20\n",
    "optimal_val_loss = np.inf\n",
    "current_patience = 0\n",
    "# Training phase\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch, target in training_generator:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_batch, val_target in validation_generator:\n",
    "                val_outputs = model(val_batch)\n",
    "                val_loss = criterion(val_outputs, val_target)\n",
    "                # Early stopping\n",
    "                if val_loss < optimal_val_loss:\n",
    "                    optimal_val_loss = val_loss\n",
    "                    torch.save(model.state_dict(), 'best_model.pt')\n",
    "                    current_patience = 0\n",
    "                else:\n",
    "                    current_patience += 1\n",
    "                    if current_patience == patience:\n",
    "                        # print(f'Early stopping at epoch {epoch+1}')\n",
    "                        # load best model\n",
    "                        model.load_state_dict(torch.load('best_model.pt'))\n",
    "                        break\n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, Val Loss: {val_loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3072.80029296875\n",
      "MSE Loss: 3072.80029296875\n",
      "R2 Score: 0.5606133704268007\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "    test_loss = criterion(predictions, y_test)\n",
    "    print(f'Test Loss: {test_loss.item()}')\n",
    "criterion = torch.nn.MSELoss()\n",
    "mse_loss = criterion(predictions, y_test)\n",
    "print(f\"MSE Loss: {mse_loss.item()}\")\n",
    "# R2 score\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"R2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YichenWang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
